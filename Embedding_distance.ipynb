{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdIvc5zKCmdn",
        "outputId": "f16a82dc-bc82-4cfa-b3e7-c64ef3026108"
      },
      "outputs": [],
      "source": [
        "### import the packages\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "from googletrans import Translator\n",
        "import expansion_utils as utils\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "import torch\n",
        "import csv\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from scipy.stats import spearmanr, kendalltau, pearsonr, ttest_ind\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import importlib\n",
        "importlib.reload(utils)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#translate Ekman emotion file into all other languages\n",
        "utils.translate_emotions(\"All_emotions//english.csv\", \"All_emotions//english_context.csv\", \"en\")\n",
        "utils.translate_emotions(\"All_emotions//english.csv\", \"All_emotions//spanish_context.csv\", \"es\")\n",
        "utils.translate_emotions(\"All_emotions//english.csv\", \"All_emotions//chinese_context.csv\", \"zh-CN\")\n",
        "utils.translate_emotions(\"All_emotions//english.csv\", \"All_emotions//japanese_context.csv\", \"ja\")\n",
        "utils.translate_emotions(\"All_emotions//english.csv\", \"All_emotions//hindi_context.csv\", \"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding(embedding_type, model_name, words):     \n",
        "    if(embedding_type == \"sentence_transformers\"):  \n",
        "        model = SentenceTransformer(model_name)\n",
        "        embeddings = model.encode(words) \n",
        "        return embeddings\n",
        "    elif(embedding_type == \"roberta\"):\n",
        "        try:\n",
        "            roberta_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "            roberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        except:\n",
        "            try:\n",
        "                roberta_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "                roberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            except:    \n",
        "                roberta_model = BertModel.from_pretrained(model_name)\n",
        "                roberta_tokenizer = BertTokenizer.from_pretrained(model_name)           \n",
        "        embeddings = []\n",
        "        for word in words:\n",
        "            sent_embedding = roberta_encoding(word, roberta_model, roberta_tokenizer)\n",
        "            embeddings.append(sent_embedding)\n",
        "        return torch.concat(embeddings, dim=0).numpy()\n",
        "  \n",
        "def roberta_encoding(sentence, roberta_model, roberta_tokenizer):\n",
        "  '''\n",
        "    Generate a mean-pooled sentence embedding using a pre-trained RoBERTa model.\n",
        "    Args:\n",
        "        sentence (str): The input sentence to be embedded.\n",
        "        model (RobertaModel): The pre-trained RoBERTa model.\n",
        "        tokenizer (RobertaTokenizer): The tokenizer corresponding to the RoBERTa model. \n",
        "    Returns:\n",
        "        mean_pooled_embedding (torch.Tensor): The mean-pooled sentence embedding.\n",
        "  '''\n",
        "  inputs = roberta_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "  with torch.no_grad():\n",
        "      outputs = roberta_model(**inputs, output_hidden_states=True)\n",
        "  token_embeddings = outputs.hidden_states[-1]\n",
        "  input_mask = inputs['attention_mask']\n",
        "  sum_embeddings = torch.sum(token_embeddings * input_mask.unsqueeze(-1), dim=1)\n",
        "  total_tokens = torch.clamp(input_mask.sum(1), min=1e-9)\n",
        "  mean_pooled_embedding = sum_embeddings / total_tokens.unsqueeze(-1)\n",
        "  return mean_pooled_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#writes to file in the form of \"emotion, embedding\"\n",
        "def encode_emotions(input_file, embedding_type, model_name, output_file):\n",
        "    emotion_list = []\n",
        "    with open((\"All_emotions//\"+input_file), newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        for row in reader:\n",
        "            emotion_list.append(row[0].lower().strip())\n",
        "            \n",
        "    embeddings = get_embedding(embedding_type, model_name, emotion_list)\n",
        "    with open((\"Encoded_Emotions/\"+output_file), 'w') as f:\n",
        "        for i in range(len(emotion_list)):\n",
        "            f.write(emotion_list[i] + \"\\t\" + str(list(embeddings[i])) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "# input: csv file in the form emotion, embedding\n",
        "# output: csv file in the form of emotion, list of nearest emotions in embedding space\n",
        "def get_nearest_emotions(input_file, output_file):\n",
        "    emotion_list = []\n",
        "    embeddings = []\n",
        "    with open((\"Encoded_Emotions/\"+input_file), newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            emotion_list.append(row[0])\n",
        "            embeddings.append(ast.literal_eval(row[1]))\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    with open((\"Nearest_Emotion_Results/\"+output_file), 'w') as f:\n",
        "        for i in range(len(emotion_list)):\n",
        "            curr_emotion = emotion_list[i]\n",
        "            f.write(curr_emotion)\n",
        "            distances = {}\n",
        "            for j in range(len(emotion_list)):\n",
        "                if(i != j):\n",
        "                    distances[emotion_list[j]] = euclidean_distances([embeddings[i]], [embeddings[j]])[0][0]\n",
        "                    # distances[emotion_list[j]] = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
        "            sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
        "            for emotion in enumerate(sorted_distances):\n",
        "                f.write(\",\" + emotion[1][0] + \": \" + str(emotion[1][1]))\n",
        "            f.write(\"\\n\")\n",
        "    print(\"DONE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create dictionary of english words --> other words\n",
        "\n",
        "def make_translation_dict(english_file, other_file):\n",
        "    english_words = []\n",
        "    other_words = []\n",
        "    with open(\"All_emotions//\"+english_file, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        for row in reader:\n",
        "            english_words.append(row[0].lower().strip())\n",
        "    with open(\"All_emotions//\"+other_file, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        for row in reader:\n",
        "            other_words.append(row[0].lower().strip())\n",
        "    assert(len(english_words) == len(other_words))\n",
        "    \n",
        "    TRANS_DICT = {}\n",
        "    for i in range(len(english_words)):\n",
        "        TRANS_DICT[english_words[i]] = other_words[i]\n",
        "    return TRANS_DICT\n",
        "\n",
        "es_trans = make_translation_dict(\"english_context.csv\", \"spanish_context.csv\")\n",
        "zh_trans = make_translation_dict(\"english_context.csv\", \"chinese_context.csv\")\n",
        "ja_trans = make_translation_dict(\"english_context.csv\", \"japanese_context.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#read in files that contain the nearest emotion for each emotion\n",
        "def compare_nearest_emotions(nearest_file1, nearest_file2):\n",
        "    emotion_list1 = []\n",
        "    nearest_list1 = []\n",
        "    with open((\"Nearest_Emotion_Results/\"+nearest_file1), newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        for row in reader:\n",
        "            emotion_list1.append(row[0])     \n",
        "            nearest_list1.append(row[1:])\n",
        "    emotion_list2 = []\n",
        "    nearest_list2 = []\n",
        "    with open((\"Nearest_Emotion_Results/\"+nearest_file2), newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        for row in reader:\n",
        "            emotion_list2.append(row[0])\n",
        "            nearest_list2.append(row[1:])\n",
        "    assert(emotion_list1 == emotion_list2)\n",
        "\n",
        "    correlation_scores = []\n",
        "    p_vals = []\n",
        "    percent_overlap = []\n",
        "    for emotion in emotion_list1:\n",
        "        nearest1 = [x.split(\":\")[0] for x in nearest_list1[emotion_list1.index(emotion)]]\n",
        "        nearest2 = [x.split(\":\")[0] for x in nearest_list2[emotion_list2.index(emotion)]]\n",
        "        \n",
        "        nearest1_unique = []\n",
        "        [nearest1_unique.append(x) for x in nearest1 if x not in nearest1_unique]\n",
        "        nearest2_unique = []\n",
        "        [nearest2_unique.append(x) for x in nearest2 if x not in nearest2_unique]\n",
        "        nearest1_unique = nearest1_unique[:50]\n",
        "        nearest2_unique = nearest2_unique[:50]\n",
        "        \n",
        "        correlation_scores.append(kendalltau(nearest1_unique, nearest2_unique)[0])\n",
        "        p_vals.append(kendalltau(nearest1_unique, nearest2_unique)[1])\n",
        "        percent_overlap.append(len(set(nearest1_unique).intersection(nearest2_unique)) / len(nearest1_unique) * 100)\n",
        "    \n",
        "    print(np.round(np.mean(percent_overlap), 2))\n",
        "\n",
        "en_ekman = [\"i feel joy\", \"i feel sadness\", \"i feel anger\", \"i feel fear\", \"i feel disgust\", \"i feel surprise\"]\n",
        "es_ekman = [es_trans[x] for x in en_ekman]\n",
        "zh_ekman = [zh_trans[x] for x in en_ekman]\n",
        "ja_ekman = [ja_trans[x] for x in en_ekman]\n",
        "ekman_all = en_ekman + es_ekman + ja_ekman + zh_ekman\n",
        "\n",
        "def get_emotion_correlation(nearest_file1, nearest_file2, mapping=False, trans_dict=None):\n",
        "    emotion_list1 = []\n",
        "    nearest_list1 = []\n",
        "    with open((\"Nearest_Emotion_Results/\"+nearest_file1), newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        for row in reader:\n",
        "            emotion_list1.append(row[0])     \n",
        "            nearest_list1.append(row[1:])\n",
        "    emotion_list2 = []\n",
        "    nearest_list2 = []\n",
        "    with open((\"Nearest_Emotion_Results/\"+nearest_file2), newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        for row in reader:\n",
        "            emotion_list2.append(row[0])\n",
        "            nearest_list2.append(row[1:])\n",
        "\n",
        "    if(mapping):\n",
        "        alphabetized_nearest_list1 = []\n",
        "        alphabetized_nearest_list2 = []\n",
        "        for i in range(len(nearest_list1)):\n",
        "            temp_emotions_en = [x.split(\":\")[0] for x in nearest_list1[i]]\n",
        "            temp_distances_en = [float(x.split(\":\")[1]) for x in nearest_list1[i]]\n",
        "            temp_emotions_other = [x.split(\":\")[0] for x in nearest_list2[i]]\n",
        "            temp_distances_other = [float(x.split(\":\")[1]) for x in nearest_list2[i]]\n",
        "\n",
        "            overlap_emotions_en = []\n",
        "            overlap_distances_en = []\n",
        "            visited_emotions = []\n",
        "            for i in range(len(temp_emotions_en)):\n",
        "                if(trans_dict[temp_emotions_en[i]] in temp_emotions_other and trans_dict[temp_emotions_en[i]] not in visited_emotions):\n",
        "                    overlap_emotions_en.append(trans_dict[temp_emotions_en[i]])\n",
        "                    overlap_distances_en.append(temp_distances_en[i])\n",
        "                    visited_emotions.append(trans_dict[temp_emotions_en[i]])\n",
        "            assert(len(overlap_emotions_en) == len(temp_emotions_other))\n",
        "\n",
        "            sorted_list1 = sorted(zip(overlap_emotions_en, overlap_distances_en))\n",
        "            sorted_list2 = sorted(zip(temp_emotions_other, temp_distances_other))\n",
        "\n",
        "            alphabetized_nearest_list1.append([sorted_list1[x][1] for x in range(len(sorted_list1))])\n",
        "            alphabetized_nearest_list2.append([sorted_list2[x][1] for x in range(len(sorted_list2))])\n",
        "    else:\n",
        "        assert(emotion_list1 == emotion_list2)\n",
        "        alphabetized_nearest_list1 = []\n",
        "        alphabetized_nearest_list2 = []\n",
        "        for l in nearest_list1:\n",
        "            sorted_list = sorted(l)\n",
        "            sorted_distances = [float(x.split(\":\")[1]) for x in sorted_list]\n",
        "            alphabetized_nearest_list1.append(sorted_distances)    \n",
        "        for l in nearest_list2:\n",
        "            sorted_list = sorted(l)\n",
        "            sorted_distances = [float(x.split(\":\")[1]) for x in sorted_list]\n",
        "            alphabetized_nearest_list2.append(sorted_distances)\n",
        "\n",
        "    assert(len(alphabetized_nearest_list1) == len(alphabetized_nearest_list2))\n",
        "    for i in range(len(alphabetized_nearest_list1)):\n",
        "        try:\n",
        "            assert(len(alphabetized_nearest_list1[i]) == len(alphabetized_nearest_list2[i]))\n",
        "        except:\n",
        "            print(len(alphabetized_nearest_list1[i]), len(alphabetized_nearest_list2[i]))\n",
        "    \n",
        "    pearson_correlation_scores = []\n",
        "    spearman_correlation_scores = []\n",
        "    pearson_p_vals = []\n",
        "    spearman_p_vals = []\n",
        "    count = 0\n",
        "    \n",
        "    visited_emotions = []\n",
        "    # print(nearest_file1.split(\"_\")[0].upper() + \"(\" + nearest_file1.split(\"_\")[1] + \") vs. \" + nearest_file2.split(\"_\")[0].upper() + \"(\" + nearest_file2.split(\"_\")[1] + \")\")\n",
        "    # print(nearest_file1.split(\"_\")[0].upper())\n",
        "    for i in range(len(emotion_list1)):\n",
        "        # if(emotion_list1[i] not in ekman_all): continue\n",
        "        if(emotion_list1[i] in visited_emotions): continue\n",
        "        else: visited_emotions.append(emotion_list1[i])\n",
        "        \n",
        "        pearson_corr = pearsonr(alphabetized_nearest_list1[i], alphabetized_nearest_list2[i])\n",
        "        spearman_corr = spearmanr(alphabetized_nearest_list1[i], alphabetized_nearest_list2[i])\n",
        "        pearson_correlation_scores.append(pearson_corr[0])\n",
        "        spearman_correlation_scores.append(spearman_corr[0])\n",
        "        # print(\"corr: %f, p-val: %s, %s\" % (pearson_corr[0], str(pearson_corr[1]), emotion_list1[i]))\n",
        "        count += 1\n",
        "    # print(\"Total emotions included: %d\" % (count))\n",
        "    print(\"Spearman Avg: %f\" % ((np.round(np.mean(spearman_correlation_scores), 3))))\n",
        "    return spearman_correlation_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Monolingual vs Multilingual\n",
        "e = get_emotion_correlation(\"english_roberta_nearest.csv\", \"english_roberta-xlm_nearest.csv\")\n",
        "s = get_emotion_correlation(\"spanish_roberta_nearest.csv\", \"spanish_roberta-xlm_nearest.csv\")\n",
        "c = get_emotion_correlation(\"chinese_roberta_nearest.csv\", \"chinese_roberta-xlm_nearest.csv\")\n",
        "j = get_emotion_correlation(\"japanese_roberta_nearest.csv\", \"japanese_roberta-xlm_nearest.csv\")\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ttest_ind(e, s, equal_var=False)[1])\n",
        "print(ttest_ind(e, c, equal_var=False)[1])\n",
        "print(ttest_ind(e, j, equal_var=False)[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#English Monolingual vs. Other Monolingual\n",
        "\n",
        "sm = get_emotion_correlation(\"english_roberta_real_nearest.csv\", \"spanish_roberta_nearest.csv\", mapping=True, trans_dict=es_trans)\n",
        "cm = get_emotion_correlation(\"english_roberta_real_nearest.csv\", \"chinese_roberta_nearest.csv\", mapping=True, trans_dict=zh_trans)\n",
        "jm = get_emotion_correlation(\"english_roberta_real_nearest.csv\", \"japanese_roberta_nearest.csv\", mapping=True, trans_dict=ja_trans)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "sx = get_emotion_correlation(\"english_roberta-xlm-sent_nearest.csv\", \"spanish_roberta-xlm-sent_nearest.csv\", mapping=True, trans_dict=es_trans)\n",
        "cx = get_emotion_correlation(\"english_roberta-xlm-sent_nearest.csv\", \"chinese_roberta-xlm-sent_nearest.csv\", mapping=True, trans_dict=zh_trans)\n",
        "jx = get_emotion_correlation(\"english_roberta-xlm-sent_nearest.csv\", \"japanese_roberta-xlm-sent_nearest.csv\", mapping=True, trans_dict=ja_trans)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(ttest_ind(sm, sx, equal_var=False)[1])\n",
        "print(ttest_ind(cm, cx, equal_var=False)[1])\n",
        "print(ttest_ind(jm, jx, equal_var=False)[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Unaligned vs. Aligned\n",
        "e = get_emotion_correlation(\"english_roberta-xlm_nearest.csv\", \"english_paraphrase-mpnet_nearest.csv\")\n",
        "s = get_emotion_correlation(\"spanish_roberta-xlm_nearest.csv\", \"spanish_paraphrase-mpnet_nearest.csv\")\n",
        "c = get_emotion_correlation(\"chinese_roberta-xlm_nearest.csv\", \"chinese_paraphrase-mpnet_nearest.csv\")\n",
        "j = get_emotion_correlation(\"japanese_roberta-xlm_nearest.csv\", \"japanese_paraphrase-mpnet_nearest.csv\")\n",
        "print('\\n')\n",
        "\n",
        "print(ttest_ind(e, s, equal_var=False)[1])\n",
        "print(ttest_ind(e, c, equal_var=False)[1])\n",
        "print(ttest_ind(e, j, equal_var=False)[1])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "shreya_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "91463280d8781b4327168e6aed37b9922ec5e51ac75b06b77d3a30bc9c56392e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
